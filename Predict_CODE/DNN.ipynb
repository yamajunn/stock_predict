{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "44/44 [==============================] - 1s 18ms/step - loss: 37.5678 - val_loss: 26.9747\n",
      "Epoch 2/200\n",
      "44/44 [==============================] - 0s 10ms/step - loss: 4.1806 - val_loss: 12.1702\n",
      "Epoch 3/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 1.7489 - val_loss: 15.8680\n",
      "Epoch 4/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.9110 - val_loss: 22.3712\n",
      "Epoch 5/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.5465 - val_loss: 42.1126\n",
      "Epoch 6/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.4039 - val_loss: 33.6363\n",
      "Epoch 7/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.2656 - val_loss: 32.8571\n",
      "Epoch 8/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.2677 - val_loss: 26.3815\n",
      "Epoch 9/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.2615 - val_loss: 41.2133\n",
      "Epoch 10/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.1920 - val_loss: 40.2716\n",
      "Epoch 11/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.1684 - val_loss: 31.7162\n",
      "Epoch 12/200\n",
      "44/44 [==============================] - 0s 9ms/step - loss: 0.1768 - val_loss: 23.1453\n",
      "83/83 [==============================] - 0s 2ms/step\n",
      "Mean Squared Error: 286.33515829664225\n",
      "Root Mean Squared Error: 16.921440786665958\n"
     ]
    }
   ],
   "source": [
    "# データを読み込む\n",
    "data_dir = '../Mining_DATA/Formatted_DATA/'\n",
    "\n",
    "# csvファイルのリストを取得\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "\n",
    "# データを格納するための辞書\n",
    "data_dict = {}\n",
    "\n",
    "# 各csvファイルを読み込む\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_dict[file] = df\n",
    "\n",
    "# AAPL.csvを除外して説明変数を作成\n",
    "appl_data = data_dict['AAPL.csv']\n",
    "\n",
    "# 同じ日付で他のCSVのDataカラムを結びつける\n",
    "merged_data = appl_data[['Date', 'Open']].copy()  # 目的変数\n",
    "\n",
    "# AAPL.csv以外のCSVファイルのDataカラムを結びつける\n",
    "for file, df in data_dict.items():\n",
    "    if file != 'AAPL.csv':\n",
    "        merged_data = pd.merge(merged_data, df[['Date', 'Open']], on='Date', suffixes=('', f'_{file[:-4]}'))\n",
    "\n",
    "merged_data.replace(0, np.nan, inplace=True)  # ゼロをNaNに変換\n",
    "merged_data.ffill(inplace=True)  # 前方向補完\n",
    "merged_data.bfill(inplace=True)  # 後方向補完\n",
    "\n",
    "# 説明変数と目的変数に分ける\n",
    "X = merged_data.drop(columns=['Date', 'Open'])  # 説明変数\n",
    "y = merged_data['Open']  # 目的変数（AAPL.csvのOpen）\n",
    "\n",
    "# データをtrainとtestに8:2で分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# データを標準化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# モデルの作成 (ニューラルネットワーク)\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(128, activation='leaky_relu'))\n",
    "model.add(Dense(256, activation='leaky_relu'))\n",
    "model.add(Dense(512, activation='leaky_relu'))\n",
    "model.add(Dense(1024, activation='leaky_relu'))\n",
    "model.add(Dense(2048, activation='leaky_relu'))\n",
    "model.add(Dense(2048, activation='leaky_relu'))\n",
    "model.add(Dense(2048, activation='leaky_relu'))\n",
    "model.add(Dense(2048, activation='leaky_relu'))\n",
    "model.add(Dense(1024, activation='leaky_relu'))\n",
    "model.add(Dense(512, activation='leaky_relu'))\n",
    "model.add(Dense(256, activation='leaky_relu'))\n",
    "model.add(Dense(128, activation='leaky_relu'))\n",
    "model.add(Dense(64, activation='leaky_relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# モデルのコンパイル\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')\n",
    "\n",
    "# 早期停止のコールバックを設定\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# モデルの学習\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    epochs=200,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping]  # コールバックを追加\n",
    ")\n",
    "\n",
    "# testデータで予測を実行\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# 精度評価\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 予測結果と実際の値をプロット\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.values, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Open Price')\n",
    "plt.title('Actual vs Predicted Open Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# データをプロットする\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# 各株のOpen値をプロット\n",
    "for file, df in data_dict.items():\n",
    "    plt.plot(df['Date'], df['Open'], label=file[:-4])\n",
    "\n",
    "# プロットの設定\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Open Price')\n",
    "plt.title('Stock Open Prices')\n",
    "plt.legend(loc='best')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# プロットを表示\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
